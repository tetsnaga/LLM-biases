{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2b4513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b57660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting networkx\n",
      "  Obtaining dependency information for networkx from https://files.pythonhosted.org/packages/eb/8d/776adee7bbf76365fdd7f2552710282c79a4ead5d2a46408c9043a2b70ba/networkx-3.5-py3-none-any.whl.metadata\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: networkx\n",
      "Successfully installed networkx-3.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e19fa",
   "metadata": {},
   "source": [
    "Nodes: agents (personas).\n",
    "\n",
    "Edges: possible social interactions (who sees whose message).\n",
    "\n",
    "Build a homophilic network:\n",
    "Start with a random graph (e.g., Erdős–Rényi or fixed degree)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef90a96",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee39a60c",
   "metadata": {},
   "source": [
    "Mean Belief (mean_belief): The average belief value across all agents. This shows the central tendency of opinions.\n",
    "\n",
    "Standard Deviation (std_belief): The spread of beliefs. A higher value means more disagreement or diversity in opinions.\n",
    "\n",
    "Fraction Extreme (frac_extreme): The proportion of agents whose beliefs are very strong (absolute value > 0.8). This measures how many agents hold extreme views.\n",
    "\n",
    "Cluster Gap (cluster_gap): If there are at least 4 agents, the code uses KMeans clustering (with 2 clusters) on the beliefs. The absolute difference between the two cluster centers is computed. A large gap suggests the population is split into two distinct groups (i.e., polarization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ac1d01",
   "metadata": {},
   "source": [
    "1. Mean Belief (mean_belief):\n",
    "\n",
    "Range: -1 to 1\n",
    "Interpretation:\n",
    "-1 = all agents strongly disagree (e.g., with climate change).\n",
    "0 = agents are, on average, neutral or evenly split.\n",
    "1 = all agents strongly agree.\n",
    "\n",
    "\n",
    "2. Standard Deviation (std_belief):\n",
    "\n",
    "Range: 0 to 1 (theoretical max, but usually less)\n",
    "Interpretation:\n",
    "0 = all agents have the same belief (no diversity).\n",
    "Higher values = more disagreement/diversity in beliefs.\n",
    "Max is 1 if half are at -1 and half at 1.\n",
    "\n",
    "\n",
    "3. Fraction Extreme (frac_extreme):\n",
    "\n",
    "Range: 0 to 1\n",
    "Interpretation:\n",
    "0 = no agents have extreme beliefs (|belief| ≤ 0.8).\n",
    "1 = all agents have extreme beliefs (|belief| > 0.8).\n",
    "Values near 1 mean most agents are at the extremes.\n",
    "\n",
    "\n",
    "\n",
    "4. Cluster Gap (cluster_gap):\n",
    "\n",
    "Range: 0 to 2\n",
    "Interpretation:\n",
    "0 = no separation between two main groups (everyone similar).\n",
    "2 = two groups at -1 and 1 (maximal polarization).\n",
    "Higher values mean two distinct camps; low values mean beliefs are mixed or unimodal.\n",
    "Summary Table:\n",
    "\n",
    "Metric\tRange\tWhat High Value Means\tWhat Low Value Means\n",
    "mean_belief\t-1 to 1\tConsensus (all agree/disagree)\tNeutral or split\n",
    "std_belief\t0 to 1\tHigh diversity/disagreement\tUniformity\n",
    "frac_extreme\t0 to 1\tMost agents are extreme\tMost are moderate/neutral\n",
    "cluster_gap\t0 to 2\tTwo opposing camps (polarization)\tNo clear split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bd186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import ollama\n",
    "from prompts import *\n",
    "\n",
    "def build_persona_description(row: pd.Series) -> str:\n",
    "    return (\n",
    "        f\"- PersonaID: {row.get('PersonaID')}\\n\"\n",
    "        f\"- AgeGroup: {row.get('AgeGroup')}\\n\"\n",
    "        f\"- Gender: {row.get('Gender')}\\n\"\n",
    "        f\"- EducationLevel: {row.get('EducationLevel')}\\n\"\n",
    "        f\"- OccupationSector: {row.get('OccupationSector')}\\n\"\n",
    "        f\"- Region: {row.get('Region')}\\n\"\n",
    "        f\"- PoliticalIdeology: {row.get('PoliticalIdeology')}\\n\"\n",
    "        f\"- Trust_ScienceInstitutions: {row.get('Trust_ScienceInstitutions')}\\n\"\n",
    "        f\"- Belief_ClimateExists: {row.get('Belief_ClimateExists')}\\n\"\n",
    "        f\"- Belief_HumanContribution: {row.get('Belief_HumanContribution')}\\n\"\n",
    "        f\"- Emotional_WorryAboutClimate: {row.get('Emotional_WorryAboutClimate')}\\n\"\n",
    "        f\"- BehaviouralOrientation: {row.get('BehaviouralOrientation')}\\n\"\n",
    "        f\"- SocialConnectivity: {row.get('SocialConnectivity')}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def stance_to_numeric(stance: str) -> float:\n",
    "    \"\"\"\n",
    "    Map Likert climateChangeStance to a numeric belief in [-1, 1].\n",
    "    You can tweak these weights later if you like.\n",
    "    \"\"\"\n",
    "    if not isinstance(stance, str):\n",
    "        return 0.0\n",
    "    s = stance.strip().lower()\n",
    "    mapping = {\n",
    "        \"strongly disagree\": -1.0,\n",
    "        \"slightly disagree\": -0.5,\n",
    "        \"neutral\": 0.0,\n",
    "        \"slightly agree\": 0.5,\n",
    "        \"strongly agree\": 1.0,\n",
    "    }\n",
    "    return mapping.get(s, 0.0)\n",
    "\n",
    "\n",
    "def initial_belief_from_persona(row: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Use Belief_ClimateExists if it's Likert-like; otherwise default to 0.\n",
    "    \"\"\"\n",
    "    val = row.get(\"Belief_ClimateExists\")\n",
    "    if isinstance(val, str):\n",
    "        return stance_to_numeric(val)\n",
    "    try:\n",
    "        v = float(val)\n",
    "        return float(np.clip(v, -1.0, 1.0))\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def load_claims_with_label_JSON(path: str) -> List[Dict[str, Any]]:\n",
    "    raw = json.loads(Path(path).read_text())\n",
    "    out = []\n",
    "    for it in raw:\n",
    "        # claim_text = (\n",
    "        #     it.get(\"claim\") or it.get(\"claim_text\") or it.get(\"statement\")\n",
    "        #     or it.get(\"text\") or \"\"\n",
    "        # )\n",
    "        claim_text = it.get(\"claim\")\n",
    "        if not claim_text:\n",
    "            raise ValueError(f\"Claim text not found in item: {it}\")\n",
    "        \n",
    "        out.append({\n",
    "            \"claim_id\": it.get(\"claim_id\") or it.get(\"id\"),\n",
    "            \"claim_text\": claim_text,\n",
    "            \"claim_stance_label\": it.get(\"label\")\n",
    "            or it.get(\"claim_label\")\n",
    "            or it.get(\"verdict\")\n",
    "            or it.get(\"stance\"),\n",
    "        })\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_claims_with_label_CSV(path: str) -> List[Dict[str, Any]]:\n",
    "    try:\n",
    "        df = pd.read_csv(Path(path))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file at {path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    raw = df.to_dict(\"records\")\n",
    "    out = []\n",
    "    for it in raw:\n",
    "        claim_text = (\n",
    "            it.get(\"claim\") or it.get(\"claim_text\") or it.get(\"statement\")\n",
    "            or it.get(\"text\") or \"\"\n",
    "        )\n",
    "        if not claim_text:\n",
    "            continue\n",
    "\n",
    "        out.append({\n",
    "            \"claim_id\": it.get(\"claim_id\") or it.get(\"id\"),\n",
    "            \"claim_text\": claim_text,\n",
    "            \"claim_stance_label\": it.get(\"stance_label\")\n",
    "        })\n",
    "    return out\n",
    "\n",
    "\n",
    "def filter_balanced_claims(claims: List[Dict[str, Any]], n_each: int = 100) -> List[Dict[str, Any]]:\n",
    "    df = pd.DataFrame(claims)\n",
    "    if \"claim_stance_label\" not in df.columns:\n",
    "        raise ValueError(\"Expected 'claim_stance_label' field in claims.\")\n",
    "\n",
    "    df[\"claim_stance_label\"] = df[\"claim_stance_label\"].astype(str).str.upper()\n",
    "\n",
    "    supports = df[df[\"claim_stance_label\"] == \"SUPPORTS\"]\n",
    "    refutes = df[df[\"claim_stance_label\"] == \"REFUTES\"]\n",
    "\n",
    "    n_each = min(n_each, len(supports), len(refutes))\n",
    "    supports_sample = supports.sample(n=n_each, random_state=42)\n",
    "    refutes_sample = refutes.sample(n=n_each, random_state=42)\n",
    "\n",
    "    balanced = pd.concat([supports_sample, refutes_sample]).sample(\n",
    "        frac=1, random_state=42\n",
    "    ).to_dict(orient=\"records\")\n",
    "    return balanced\n",
    "\n",
    "\n",
    "def coerce_json(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Your forgiving JSON parser, slightly cleaned.\n",
    "    \"\"\"\n",
    "    text = text.strip().strip(\"`\").replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "    s, e = text.find(\"{\"), text.rfind(\"}\")\n",
    "    if s != -1 and e != -1:\n",
    "        text = text[s:e + 1]\n",
    "\n",
    "    # Try JSON parse\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        # fallback simple regex\n",
    "        stance_match = re.search(\n",
    "            r'\"climateChangeStance\"\\s*:\\s*\"([^\"]+)\"', text)\n",
    "        claim_match = re.search(r'\"claimStance\"\\s*:\\s*\"([^\"]+)\"', text)\n",
    "        return {\n",
    "            \"climateChangeStance\": stance_match.group(1) if stance_match else \"Neutral\",\n",
    "            \"claimStance\": claim_match.group(1) if claim_match else \"Not Support\",\n",
    "        }\n",
    "\n",
    "def chat_once(model: str, temperature: float, system_msg: str, user_msg: str) -> str:\n",
    "    r = ollama.chat(\n",
    "        model=model,\n",
    "        options={\"temperature\": temperature},\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ],\n",
    "    )\n",
    "    return r[\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "def chat_seq(model: str, temperature: float, messages: List[Dict[str, str]]) -> str:\n",
    "    r = ollama.chat(\n",
    "        model=model,\n",
    "        options={\"temperature\": temperature},\n",
    "        messages=messages,\n",
    "    )\n",
    "    return r[\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    One persona-based LLM agent.\n",
    "    Belief is inferred from last climateChangeStance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, idx: int, row: pd.Series):\n",
    "        self.idx = idx\n",
    "        self.persona_id = row.get(\"PersonaID\", f\"persona_{idx}\")\n",
    "        self.name = f\"Persona_{self.persona_id}\"\n",
    "        self.persona_desc = build_persona_description(row)\n",
    "        self.current_belief = initial_belief_from_persona(row)\n",
    "        self.history: List[Dict[str, Any]] = []  # one entry per time step\n",
    "\n",
    "    def last_stance_text(self) -> Optional[str]:\n",
    "        if not self.history:\n",
    "            return None\n",
    "        h = self.history[-1]\n",
    "        return h.get(\"llm_response_raw\")\n",
    "\n",
    "    def last_stance_struct(self) -> Optional[Dict[str, Any]]:\n",
    "        if not self.history:\n",
    "            return None\n",
    "        return self.history[-1].get(\"llm_response\")\n",
    "\n",
    "\n",
    "def build_fully_connected_graph(n_agents: int) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Every agent sees every other agent.\n",
    "    Neutral, maximally mixed environment.\n",
    "    \"\"\"\n",
    "    G = nx.complete_graph(n_agents)\n",
    "    return G\n",
    "\n",
    "\n",
    "def build_random_graph(n_agents: int, avg_degree: int = 4, seed: int = 42) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Erdős–Rényi random graph, no bias from beliefs.\n",
    "    \"\"\"\n",
    "    p = avg_degree / max(n_agents - 1, 1)\n",
    "    G = nx.erdos_renyi_graph(n_agents, p, seed=seed)\n",
    "\n",
    "    # ensure connectivity (optional but nice)\n",
    "    if not nx.is_connected(G):\n",
    "        components = list(nx.connected_components(G))\n",
    "        for c1, c2 in zip(components[:-1], components[1:]):\n",
    "            i = next(iter(c1))\n",
    "            j = next(iter(c2))\n",
    "            G.add_edge(i, j)\n",
    "    return G\n",
    "\n",
    "\n",
    "def build_small_world_graph(\n",
    "    n_agents: int, k: int = 4, beta: float = 0.1, seed: int = 42\n",
    ") -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Watts–Strogatz small-world graph.\n",
    "    Also neutral w.r.t. beliefs.\n",
    "    k = each node is connected to k nearest neighbors in a ring (must be even).\n",
    "    beta = rewiring probability.\n",
    "    \"\"\"\n",
    "    if k % 2 == 1:\n",
    "        k += 1  # enforce even\n",
    "    if k >= n_agents:\n",
    "        k = max(2, n_agents - 1)\n",
    "        if k % 2 == 1:\n",
    "            k -= 1\n",
    "    G = nx.watts_strogatz_graph(n_agents, k, beta, seed=seed)\n",
    "    return G\n",
    "\n",
    "\n",
    "\n",
    "def summarize_neighbors(\n",
    "    agent_idx: int,\n",
    "    agents: List[Agent],\n",
    "    G: nx.Graph,\n",
    "    max_neighbors: int = 3,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build a short text summary of neighbors' last stances.\n",
    "    This is what you feed into the user prompt as 'social context'.\n",
    "    \"\"\"\n",
    "    neighbors = list(G.neighbors(agent_idx))\n",
    "    if not neighbors:\n",
    "        return \"No one in your social circle has expressed an opinion yet.\"\n",
    "\n",
    "    # Take up to max_neighbors neighbors that actually have history\n",
    "    candidates = [j for j in neighbors if agents[j].history]\n",
    "    if not candidates:\n",
    "        return \"No one in your social circle has expressed an opinion yet.\"\n",
    "\n",
    "    random.shuffle(candidates)\n",
    "    chosen = candidates[:max_neighbors]\n",
    "\n",
    "    lines = []\n",
    "    for j in chosen:\n",
    "        h = agents[j].history[-1]\n",
    "        n_name = agents[j].name\n",
    "        llm_struct = h.get(\"llm_response\") or {}\n",
    "        cstance = llm_struct.get(\"climateChangeStance\", \"Neutral\")\n",
    "        claimstance = llm_struct.get(\"claimStance\", \"Support\")\n",
    "        lines.append(\n",
    "            f\"- {n_name} evaluated a similar climate-related claim and had climateChangeStance='{cstance}' \"\n",
    "            f\"and claimStance='{claimstance}'.\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(lines) if lines else \"No one in your social circle has expressed an opinion yet.\"\n",
    "\n",
    "def simulate_polarization(\n",
    "    model: str,\n",
    "    personas_df: pd.DataFrame,\n",
    "    claims: List[Dict[str, Any]],\n",
    "    graph_type: str = \"fully_connected\",\n",
    "    steps: int = 10,\n",
    "    temperature: float = 0.2,\n",
    "    avg_degree: int = 4,\n",
    "    small_world_k: int = 4,\n",
    "    small_world_beta: float = 0.1,\n",
    "    seed: int = 42,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run multi-agent simulation with 3 graph options.\n",
    "    Returns a log DataFrame (one row per agent per time step).\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Initialize agents\n",
    "    agents: List[Agent] = [\n",
    "        Agent(idx=i, row=row) for i, (_, row) in enumerate(personas_df.iterrows())\n",
    "    ]\n",
    "    n_agents = len(agents)\n",
    "\n",
    "    # Build graph\n",
    "    if graph_type == \"fully_connected\":\n",
    "        G = build_fully_connected_graph(n_agents)\n",
    "    elif graph_type == \"random\":\n",
    "        G = build_random_graph(n_agents, avg_degree=avg_degree, seed=seed)\n",
    "    elif graph_type == \"small_world\":\n",
    "        G = build_small_world_graph(\n",
    "            n_agents, k=small_world_k, beta=small_world_beta, seed=seed)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown graph_type '{graph_type}'. Use one of: fully_connected, random, small_world\")\n",
    "\n",
    "    print(f\"Graph '{graph_type}' -> nodes={G.number_of_nodes()}, edges={G.number_of_edges()}\")\n",
    "\n",
    "    logs: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Cycle through claims\n",
    "    if not claims:\n",
    "        raise ValueError(\"No claims loaded.\")\n",
    "\n",
    "    for t in range(steps):\n",
    "        claim = claims[t % len(claims)]\n",
    "        claim_id = claim.get(\"claim_id\")\n",
    "        claim_text = str(claim.get(\"claim_text\"))\n",
    "\n",
    "        for agent in tqdm(agents, desc=f\"Time step {t}\", leave=False):\n",
    "            # Build system prompt for this persona\n",
    "            system_msg = GROUP_SYSTEM_TMPL.replace(\n",
    "                \"{PERSONA_DESCRIPTION}\", agent.persona_desc\n",
    "            )\n",
    "\n",
    "            # Build neighbor summary\n",
    "            neighbor_summary = summarize_neighbors(\n",
    "                agent.idx, agents, G, max_neighbors=3\n",
    "            )\n",
    "            if \"No one in your social circle\" in neighbor_summary:\n",
    "                user_prompt = USER_TMPL_NO_NEIGHBORS.replace(\n",
    "                    \"{CLAIM_TEXT}\", claim_text\n",
    "                )\n",
    "            else:\n",
    "                user_prompt = USER_TMPL_WITH_NEIGHBORS.replace(\n",
    "                    \"{CLAIM_TEXT}\", claim_text\n",
    "                ).replace(\"{NEIGHBOR_SUMMARY}\", neighbor_summary)\n",
    "\n",
    "            # Call model\n",
    "            raw = chat_once(\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                system_msg=system_msg,\n",
    "                user_msg=user_prompt\n",
    "                + \"\\n\\nONLY return the JSON above — no words or explanation.\",\n",
    "            )\n",
    "\n",
    "            parsed = coerce_json(raw)\n",
    "            new_belief = stance_to_numeric(parsed.get(\"climateChangeStance\", \"Neutral\"))\n",
    "            agent.current_belief = new_belief\n",
    "\n",
    "            # Append to agent history\n",
    "            step_record = {\n",
    "                \"time\": t,\n",
    "                \"persona_id\": agent.persona_id,\n",
    "                \"agent_idx\": agent.idx,\n",
    "                \"name\": agent.name,\n",
    "                \"claim_id\": claim_id,\n",
    "                \"claim_text\": claim_text,\n",
    "                \"graph_type\": graph_type,\n",
    "                \"climateChangeStance\": parsed.get(\"climateChangeStance\"),\n",
    "                \"claimStance\": parsed.get(\"claimStance\"),\n",
    "                \"belief_numeric\": agent.current_belief,\n",
    "                \"llm_response\": parsed,\n",
    "                \"llm_response_raw\": raw,\n",
    "                \"neighbor_summary\": neighbor_summary,\n",
    "            }\n",
    "            agent.history.append(step_record)\n",
    "            logs.append(step_record)\n",
    "\n",
    "    df_logs = pd.DataFrame(logs)\n",
    "    return df_logs\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. Polarization metrics\n",
    "# ============================================================\n",
    "\n",
    "def compute_polarization_metrics(df_logs: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute simple polarization metrics per time step from df_logs.\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    for t, df_t in df_logs.groupby(\"time\"):\n",
    "        beliefs = df_t[\"belief_numeric\"].values.reshape(-1, 1)\n",
    "        mean = float(beliefs.mean())\n",
    "        std = float(beliefs.std())\n",
    "        frac_extreme = float(np.mean(np.abs(beliefs) > 0.8))\n",
    "\n",
    "        # cluster gap (two-cluster separation)\n",
    "        if len(beliefs) >= 4:\n",
    "            km = KMeans(n_clusters=2, n_init=10, random_state=0)\n",
    "            km.fit(beliefs)\n",
    "            m1, m2 = km.cluster_centers_.flatten()\n",
    "            cluster_gap = float(abs(m1 - m2))\n",
    "        else:\n",
    "            cluster_gap = float(\"nan\")\n",
    "\n",
    "        metrics.append({\n",
    "            \"time\": t,\n",
    "            \"mean_belief\": mean,\n",
    "            \"std_belief\": std,\n",
    "            \"frac_extreme\": frac_extreme,\n",
    "            \"cluster_gap\": cluster_gap,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. CLI main\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Multi-agent climate polarization simulation with LLM personas.\")\n",
    "    parser.add_argument(\"--model\", required=True, help=\"Ollama model name\")\n",
    "    parser.add_argument(\"--personas\", required=True, help=\"Path to personas CSV file\")\n",
    "    parser.add_argument(\"--claims\", required=True, help=\"Path to claims CSV or JSON file\")\n",
    "    parser.add_argument(\n",
    "        \"--claims_format\",\n",
    "        choices=[\"csv\", \"json\"],\n",
    "        default=\"csv\",\n",
    "        help=\"Format of claims file\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--graph_type\",\n",
    "        choices=[\"fully_connected\", \"random\", \"small_world\"],\n",
    "        default=\"fully_connected\",\n",
    "        help=\"Interaction graph type\",\n",
    "    )\n",
    "    parser.add_argument(\"--steps\", type=int, default=10, help=\"Number of time steps\")\n",
    "    parser.add_argument(\"--temperature\", type=float, default=0.2, help=\"Sampling temperature\")\n",
    "    parser.add_argument(\"--avg_degree\", type=int, default=4, help=\"Avg degree for random graph\")\n",
    "    parser.add_argument(\"--small_world_k\", type=int, default=4, help=\"k for small-world\")\n",
    "    parser.add_argument(\"--small_world_beta\", type=float, default=0.1, help=\"beta for small-world\")\n",
    "    parser.add_argument(\"--n_personas\", type=int, default=None, help=\"Subset of personas\")\n",
    "    parser.add_argument(\"--n_claims\", type=int, default=50, help=\"Number of claims to use\")\n",
    "    parser.add_argument(\"--balanced_claims\", action=\"store_true\",\n",
    "                        help=\"If set, balance SUPPORTS / REFUTES (only for labeled datasets)\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "    parser.add_argument(\"--out_prefix\", default=\"group_polarization\", help=\"Output prefix\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Personas\n",
    "    personas_df = pd.read_csv(args.personas)\n",
    "    if args.n_personas is not None:\n",
    "        personas_df = personas_df.sample(\n",
    "            n=min(args.n_personas, len(personas_df)),\n",
    "            random_state=args.seed)\n",
    "\n",
    "    # Claims\n",
    "    if args.claims_format == \"csv\":\n",
    "        claims = load_claims_with_label_CSV(args.claims)\n",
    "    else:\n",
    "        claims = load_claims_with_label_JSON(args.claims)\n",
    "\n",
    "    if args.balanced_claims:\n",
    "        claims = filter_balanced_claims(claims, n_each=args.n_claims // 2)\n",
    "    else:\n",
    "        # Just sample without balancing\n",
    "        if args.n_claims is not None and args.n_claims < len(claims):\n",
    "            claims = (\n",
    "                pd.DataFrame(claims)\n",
    "                .sample(n=args.n_claims, random_state=args.seed)\n",
    "                .to_dict(orient=\"records\"))\n",
    "\n",
    "    print(f\"Loaded {len(personas_df)} personas and {len(claims)} claims.\")\n",
    "\n",
    "    df_logs = simulate_polarization(\n",
    "        model=args.model,\n",
    "        personas_df=personas_df,\n",
    "        claims=claims,\n",
    "        graph_type=args.graph_type,\n",
    "        steps=args.steps,\n",
    "        temperature=args.temperature,\n",
    "        avg_degree=args.avg_degree,\n",
    "        small_world_k=args.small_world_k,\n",
    "        small_world_beta=args.small_world_beta,\n",
    "        seed=args.seed,\n",
    "    )\n",
    "\n",
    "    df_metrics = compute_polarization_metrics(df_logs)\n",
    "\n",
    "    out_dir = Path(\"outputs\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logs_path = out_dir / f\"{args.out_prefix}_logs.jsonl\"\n",
    "    metrics_path = out_dir / f\"{args.out_prefix}_metrics.csv\"\n",
    "\n",
    "    # Save logs as JSONL\n",
    "    with logs_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in df_logs.iterrows():\n",
    "            f.write(json.dumps(row.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # Save metrics\n",
    "    df_metrics.to_csv(metrics_path, index=False)\n",
    "\n",
    "    print(f\"Saved logs to {logs_path}\")\n",
    "    print(f\"Saved metrics to {metrics_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f429590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e1e3be0",
   "metadata": {},
   "source": [
    "we need to aggregate the beliefs of all agents at each time step  and give it to the next step, instead of only giving the last belief of each neighbor to the next one\n",
    "\n",
    "what should we give in the promot about the neighbours:\n",
    "- just avg of score\n",
    "- each person score + rationales \n",
    "- each person score + rationales  +persoa\n",
    "- each person score + persoa\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb6571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import ollama\n",
    "from prompts import *\n",
    "\n",
    "def build_persona_description(row: pd.Series) -> str:\n",
    "    return (\n",
    "        f\"- PersonaID: {row.get('PersonaID')}\\n\"\n",
    "        f\"- AgeGroup: {row.get('AgeGroup')}\\n\"\n",
    "        f\"- Gender: {row.get('Gender')}\\n\"\n",
    "        f\"- EducationLevel: {row.get('EducationLevel')}\\n\"\n",
    "        f\"- OccupationSector: {row.get('OccupationSector')}\\n\"\n",
    "        f\"- Region: {row.get('Region')}\\n\"\n",
    "        f\"- PoliticalIdeology: {row.get('PoliticalIdeology')}\\n\"\n",
    "        f\"- Trust_ScienceInstitutions: {row.get('Trust_ScienceInstitutions')}\\n\"\n",
    "        f\"- Belief_ClimateExists: {row.get('Belief_ClimateExists')}\\n\"\n",
    "        f\"- Belief_HumanContribution: {row.get('Belief_HumanContribution')}\\n\"\n",
    "        f\"- Emotional_WorryAboutClimate: {row.get('Emotional_WorryAboutClimate')}\\n\"\n",
    "        f\"- BehaviouralOrientation: {row.get('BehaviouralOrientation')}\\n\"\n",
    "        f\"- SocialConnectivity: {row.get('SocialConnectivity')}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def stance_to_numeric(stance: str) -> float:\n",
    "    \"\"\"\n",
    "    Map various belief strings to numeric belief in [-1, 1].\n",
    "    Accepts either Likert climateChangeStance strings or the new\n",
    "    climateChange_belief labels.\n",
    "    \"\"\"\n",
    "    if not isinstance(stance, str):\n",
    "        return 0.0\n",
    "    s = stance.strip().lower()\n",
    "    mapping = {\n",
    "        \"strongly disagree.\": -1.0,\n",
    "        \"slightly disagree.\": -0.5,\n",
    "        \"neutral.\": 0.0,\n",
    "        \"slightly agree.\": 0.5,\n",
    "        \"strongly agree.\": 1.0\n",
    "    }\n",
    "\n",
    "    if s in mapping:\n",
    "        return mapping[s]\n",
    "    # try some fuzzy matching\n",
    "    if \"strong\" in s and \"disagree\" in s:\n",
    "        return -1.0\n",
    "    if \"slight\" in s and \"disagree\" in s:\n",
    "        return -0.5\n",
    "    if s.startswith(\"neutral\"):\n",
    "        return 0.0\n",
    "    if \"slight\" in s and \"agree\" in s:\n",
    "        return 0.5\n",
    "    if \"strong\" in s and \"agree\" in s:\n",
    "        return 1.0\n",
    "    # fallback numeric parse\n",
    "    try:\n",
    "        v = float(s)\n",
    "        return float(np.clip(v, -1.0, 1.0))\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def initial_belief_from_persona(row: pd.Series) -> float:\n",
    "    val = row.get(\"Belief_ClimateExists\")\n",
    "    if isinstance(val, str):\n",
    "        return stance_to_numeric(val)\n",
    "    v = float(val)\n",
    "    return float(np.clip(v, -1.0, 1.0))\n",
    "\n",
    "\n",
    "\n",
    "def load_claims_with_label_JSON(path: str) -> List[Dict[str, Any]]:\n",
    "    raw = json.loads(Path(path).read_text())\n",
    "    out = []\n",
    "    for it in raw:\n",
    "        claim_text = it.get(\"claim\")\n",
    "        if not claim_text:\n",
    "            raise ValueError(f\"Claim text not found in item: {it}\")\n",
    "        \n",
    "        out.append({\n",
    "            \"claim_id\": it.get(\"claim_id\") or it.get(\"id\"),\n",
    "            \"claim_text\": claim_text,\n",
    "            \"claim_stance_label\": it.get(\"label\")\n",
    "            or it.get(\"claim_label\")\n",
    "            or it.get(\"verdict\")\n",
    "            or it.get(\"stance\"),\n",
    "        })\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_claims_with_label_CSV(path: str) -> List[Dict[str, Any]]:\n",
    "    try:\n",
    "        df = pd.read_csv(Path(path))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file at {path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    raw = df.to_dict(\"records\")\n",
    "    out = []\n",
    "    for it in raw:\n",
    "        claim_text = (\n",
    "            it.get(\"claim\") or it.get(\"claim_text\") or it.get(\"statement\")\n",
    "            or it.get(\"text\") or \"\"\n",
    "        )\n",
    "        if not claim_text:\n",
    "            continue\n",
    "\n",
    "        out.append({\n",
    "            \"claim_id\": it.get(\"claim_id\") or it.get(\"id\"),\n",
    "            \"claim_text\": claim_text,\n",
    "            \"claim_stance_label\": it.get(\"stance_label\")\n",
    "        })\n",
    "    return out\n",
    "\n",
    "\n",
    "def filter_balanced_claims(claims: List[Dict[str, Any]], n_each: int = 100) -> List[Dict[str, Any]]:\n",
    "    df = pd.DataFrame(claims)\n",
    "    if \"claim_stance_label\" not in df.columns:\n",
    "        raise ValueError(\"Expected 'claim_stance_label' field in claims.\")\n",
    "\n",
    "    df[\"claim_stance_label\"] = df[\"claim_stance_label\"].astype(str).str.upper()\n",
    "\n",
    "    supports = df[df[\"claim_stance_label\"] == \"SUPPORTS\"]\n",
    "    refutes = df[df[\"claim_stance_label\"] == \"REFUTES\"]\n",
    "\n",
    "    n_each = min(n_each, len(supports), len(refutes))\n",
    "    supports_sample = supports.sample(n=n_each, random_state=42)\n",
    "    refutes_sample = refutes.sample(n=n_each, random_state=42)\n",
    "\n",
    "    balanced = pd.concat([supports_sample, refutes_sample]).sample(\n",
    "        frac=1, random_state=42\n",
    "    ).to_dict(orient=\"records\")\n",
    "    return balanced\n",
    "\n",
    "\n",
    "def coerce_json(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Forgiving JSON parser for the agent's response.\n",
    "    Expects the structure:\n",
    "    {\n",
    "      \"claim_decision\":\"<Accept | Neutral | Refute>\",\n",
    "      \"claim_decision_reason\":\"...\",\n",
    "      \"climateChange_belief\":\"<Strongly disagree | Slightly Disagree | Neutral | Slightly Agree | Strongly Agree>\",\n",
    "      \"climateChange_belief_reason\":\"...\"\n",
    "    }\n",
    "    Falls back to extracting fields by regex if strict JSON parsing fails.\n",
    "    \"\"\"\n",
    "    text_clean = text.strip().strip(\"`\").replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "    s, e = text_clean.find(\"{\"), text_clean.rfind(\"}\")\n",
    "    if s != -1 and e != -1:\n",
    "        text_clean = text_clean[s:e + 1]\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(text_clean)\n",
    "        # normalize keys (some prompts may return slight variants)\n",
    "        out = {\n",
    "            \"claim_decision\": parsed.get(\"claim_decision\") or parsed.get(\"claimDecision\") or parsed.get(\"claim_stance\") or parsed.get(\"claimStance\"),\n",
    "            \"claim_decision_reason\": parsed.get(\"claim_decision_reason\") or parsed.get(\"claimDecisionReason\") or parsed.get(\"claim_decision_reasoning\") or parsed.get(\"reason\"),\n",
    "            \"climateChange_belief\": parsed.get(\"climateChange_belief\") or parsed.get(\"climateChange_belief_label\") or parsed.get(\"climateChangeBelief\") or parsed.get(\"climateChangeStance\"),\n",
    "            \"climateChange_belief_reason\": parsed.get(\"climateChange_belief_reason\") or parsed.get(\"climateChangeBeliefReason\") or parsed.get(\"climateChange_belief_reasoning\"),\n",
    "        }\n",
    "        # fill defaults\n",
    "        out = {k: (v if v is not None else \"\") for k, v in out.items()}\n",
    "        return out\n",
    "    except Exception:\n",
    "        # fallback: try to extract fields via regex\n",
    "        def rex(key):\n",
    "            m = re.search(rf'\"{key}\"\\s*:\\s*\"([^\"]+)\"', text_clean, re.IGNORECASE)\n",
    "            return m.group(1) if m else None\n",
    "\n",
    "        claim_decision = rex(\"claim_decision\") or rex(\"claimDecision\") or rex(\"claim_stance\") or rex(\"claimStance\")\n",
    "        claim_decision_reason = rex(\"claim_decision_reason\") or rex(\"claimDecisionReason\") or rex(\"reason\")\n",
    "        climateChange_belief = rex(\"climateChange_belief\") or rex(\"climateChangeStance\") or rex(\"climateChangeBelief\")\n",
    "        climateChange_belief_reason = rex(\"climateChange_belief_reason\") or rex(\"climateChangeBeliefReason\")\n",
    "\n",
    "        return {\n",
    "            \"claim_decision\": claim_decision or \"Neutral\",\n",
    "            \"claim_decision_reason\": claim_decision_reason or \"\",\n",
    "            \"climateChange_belief\": climateChange_belief or \"Neutral\",\n",
    "            \"climateChange_belief_reason\": climateChange_belief_reason or \"\",\n",
    "        }\n",
    "\n",
    "\n",
    "def chat_once(model: str, temperature: float, system_msg: str, user_msg: str) -> str:\n",
    "    r = ollama.chat(\n",
    "        model=model,\n",
    "        options={\"temperature\": temperature},\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ],\n",
    "    )\n",
    "    return r[\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "def chat_seq(model: str, temperature: float, messages: List[Dict[str, str]]) -> str:\n",
    "    r = ollama.chat(\n",
    "        model=model,\n",
    "        options={\"temperature\": temperature},\n",
    "        messages=messages,\n",
    "    )\n",
    "    return r[\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    One persona-based LLM agent.\n",
    "    Belief is inferred from last climateChange_belief.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, idx: int, row: pd.Series):\n",
    "        self.idx = idx\n",
    "        self.persona_id = row.get(\"PersonaID\", f\"persona_{idx}\")\n",
    "        self.persona_desc = build_persona_description(row)\n",
    "        self.current_belief = initial_belief_from_persona(row)\n",
    "        self.history: List[Dict[str, Any]] = []  # one entry per phase (phase1/phase2) per time step\n",
    "\n",
    "    def last_response(self) -> Optional[Dict[str, Any]]:\n",
    "        if not self.history:\n",
    "            return None\n",
    "        return self.history[-1].get(\"llm_response_struct\")\n",
    "\n",
    "\n",
    "def build_fully_connected_graph(n_agents: int) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Every agent sees every other agent.\n",
    "    Neutral, maximally mixed environment.\n",
    "    \"\"\"\n",
    "    G = nx.complete_graph(n_agents)\n",
    "    return G\n",
    "\n",
    "\n",
    "def build_random_graph(n_agents: int, avg_degree: int = 4, seed: int = 42) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Erdős–Rényi random graph, no bias from beliefs.\n",
    "    \"\"\"\n",
    "    p = avg_degree / max(n_agents - 1, 1)\n",
    "    G = nx.erdos_renyi_graph(n_agents, p, seed=seed)\n",
    "\n",
    "    # ensure connectivity (optional but nice)\n",
    "    if not nx.is_connected(G):\n",
    "        components = list(nx.connected_components(G))\n",
    "        for c1, c2 in zip(components[:-1], components[1:]):\n",
    "            i = next(iter(c1))\n",
    "            j = next(iter(c2))\n",
    "            G.add_edge(i, j)\n",
    "    return G\n",
    "\n",
    "\n",
    "def build_small_world_graph(\n",
    "    n_agents: int, k: int = 4, beta: float = 0.1, seed: int = 42\n",
    ") -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Watts–Strogatz small-world graph.\n",
    "    Also neutral w.r.t. beliefs.\n",
    "    k = each node is connected to k nearest neighbors in a ring (must be even).\n",
    "    beta = rewiring probability.\n",
    "    \"\"\"\n",
    "    if k % 2 == 1:\n",
    "        k += 1  # enforce even\n",
    "    if k >= n_agents:\n",
    "        k = max(2, n_agents - 1)\n",
    "        if k % 2 == 1:\n",
    "            k -= 1\n",
    "    G = nx.watts_strogatz_graph(n_agents, k, beta, seed=seed)\n",
    "    return G\n",
    "\n",
    "\n",
    "def summarize_neighbors_all(\n",
    "    agent_idx: int,\n",
    "    agents: List[Agent],\n",
    "    G: nx.Graph,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Return a multi-line string containing ALL neighbors' Phase-1 (or latest) responses:\n",
    "    Each line includes neighbor id, name, claim_decision and its reason, and climateChange_belief and its reason.\n",
    "\n",
    "    If no neighbor has expressed an opinion yet, returns an explicit sentence.\n",
    "    \"\"\"\n",
    "    neighbors = list(G.neighbors(agent_idx))\n",
    "    if not neighbors:\n",
    "        return \"No one in your social circle has expressed an opinion yet.\"\n",
    "\n",
    "    # Keep ordering deterministic for reproducibility\n",
    "    neighbors = sorted(neighbors)\n",
    "\n",
    "    lines = []\n",
    "    any_with_history = False\n",
    "    for j in neighbors:\n",
    "        if not agents[j].history:\n",
    "            # skip neighbors who never responded yet\n",
    "            continue\n",
    "        any_with_history = True\n",
    "        # take the last recorded response struct (could be phase1 or phase2)\n",
    "        h = agents[j].history[-1]\n",
    "        llm_struct = h.get(\"llm_response_struct\") or {}\n",
    "        cdecision = llm_struct.get(\"claim_decision\", \"Neutral\")\n",
    "        cdecision_reason = llm_struct.get(\"claim_decision_reason\", \"\").strip()\n",
    "        cbelief = llm_struct.get(\"climateChange_belief\", \"Neutral\")\n",
    "        cbelief_reason = llm_struct.get(\"climateChange_belief_reason\", \"\").strip()\n",
    "\n",
    "        lines.append(\n",
    "            f\"[Neighbor {j}] {agents[j].name}: claim_decision='{cdecision}' (why: {cdecision_reason}) ; \"\n",
    "            f\"belief='{cbelief}' (why: {cbelief_reason})\"\n",
    "        )\n",
    "\n",
    "    if not any_with_history:\n",
    "        return \"No one in your social circle has expressed an opinion yet.\"\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def simulate_polarization(\n",
    "    model: str,\n",
    "    personas_df: pd.DataFrame,\n",
    "    claims: List[Dict[str, Any]],\n",
    "    graph_type: str = \"fully_connected\",\n",
    "    steps: int = 10,\n",
    "    temperature: float = 0.2,\n",
    "    avg_degree: int = 4,\n",
    "    small_world_k: int = 4,\n",
    "    small_world_beta: float = 0.1,\n",
    "    seed: int = 42,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run multi-agent simulation with two-phase rounds:\n",
    "    Phase 1: give claim[t] to all agents, collect responses.\n",
    "    Phase 2: give each agent the full set of Phase 1 responses (including their own),\n",
    "             then present claim[t+1] (or same if not available) and collect follow-up responses.\n",
    "    Returns a log DataFrame (one row per agent per phase per time step).\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Initialize agents\n",
    "    agents: List[Agent] = [\n",
    "        Agent(idx=i, row=row) for i, (_, row) in enumerate(personas_df.iterrows())\n",
    "    ]\n",
    "    n_agents = len(agents)\n",
    "\n",
    "    # Build graph\n",
    "    if graph_type == \"fully_connected\":\n",
    "        G = build_fully_connected_graph(n_agents)\n",
    "    elif graph_type == \"random\":\n",
    "        G = build_random_graph(n_agents, avg_degree=avg_degree, seed=seed)\n",
    "    elif graph_type == \"small_world\":\n",
    "        G = build_small_world_graph(\n",
    "            n_agents, k=small_world_k, beta=small_world_beta, seed=seed)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown graph_type '{graph_type}'. Use one of: fully_connected, random, small_world\")\n",
    "\n",
    "    logs: List[Dict[str, Any]] = []\n",
    "\n",
    "    if not claims:\n",
    "        raise ValueError(\"No claims loaded.\")\n",
    "\n",
    "    # We'll iterate steps times; each step uses claim t for Phase1 and claim t+1 for Phase2 (wrap-around)\n",
    "    for t in range(steps):\n",
    "        claim = claims[t % len(claims)]\n",
    "        claim_id = claim.get(\"claim_id\")\n",
    "        claim_text = str(claim.get(\"claim_text\"))\n",
    "\n",
    "        # -------------------------\n",
    "        # Phase 1: give claim to all agents and collect responses\n",
    "        # -------------------------\n",
    "        phase1_responses: Dict[int, Dict[str, Any]] = {}\n",
    "        for agent in tqdm(agents, desc=f\"Time {t} Phase 1\", leave=False):\n",
    "            system_msg = GROUP_SYSTEM_TMPL.replace(\"{PERSONA_DESCRIPTION}\", agent.persona_desc)\n",
    "\n",
    "            user_prompt = FIRST_ROUND_PROMPT.format(\n",
    "                CLAIM_TEXT=claim_text)\n",
    "\n",
    "            raw = chat_once(\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                system_msg=system_msg,\n",
    "                user_msg=user_prompt\n",
    "            )\n",
    "\n",
    "            parsed = coerce_json(raw)\n",
    "            # numeric belief\n",
    "            new_belief = stance_to_numeric(parsed.get(\"climateChange_belief\", \"Neutral\"))\n",
    "            agent.current_belief = new_belief\n",
    "\n",
    "            # record response struct\n",
    "            phase1_responses[agent.idx] = {\n",
    "                \"agent_idx\": agent.idx,\n",
    "                \"persona_id\": agent.persona_id,\n",
    "                \"name\": agent.name,\n",
    "                \"claim_id\": claim_id,\n",
    "                \"claim_text\": claim_text,\n",
    "                \"llm_response_raw\": raw,\n",
    "                \"llm_response_struct\": parsed,\n",
    "                \"belief_numeric\": agent.current_belief,\n",
    "            }\n",
    "\n",
    "            # append to agent history\n",
    "            agent.history.append({\n",
    "                \"time\": t,\n",
    "                \"phase\": 1,\n",
    "                \"llm_response\": parsed,\n",
    "                \"llm_response_struct\": parsed,\n",
    "                \"llm_response_raw\": raw,\n",
    "                \"claim_id\": claim_id,\n",
    "                \"claim_text\": claim_text,\n",
    "            })\n",
    "\n",
    "            # log row for phase 1 (we'll augment with majority info after collecting all)\n",
    "            logs.append({\n",
    "                \"time\": t,\n",
    "                \"phase\": 1,\n",
    "                \"persona_id\": agent.persona_id,\n",
    "                \"agent_idx\": agent.idx,\n",
    "                \"name\": agent.name,\n",
    "                \"claim_id\": claim_id,\n",
    "                \"claim_text\": claim_text,\n",
    "                \"graph_type\": graph_type,\n",
    "                \"claim_decision\": parsed.get(\"claim_decision\"),\n",
    "                \"claim_decision_reason\": parsed.get(\"claim_decision_reason\"),\n",
    "                \"climateChange_belief\": parsed.get(\"climateChange_belief\"),\n",
    "                \"climateChange_belief_reason\": parsed.get(\"climateChange_belief_reason\"),\n",
    "                \"belief_numeric\": agent.current_belief,\n",
    "                \"llm_response_raw\": raw,\n",
    "            })\n",
    "\n",
    "        # After Phase 1 for this time t, compute majority (>2/3) of claim_decision\n",
    "        decisions = [phase1_responses[i][\"llm_response_struct\"].get(\"claim_decision\", \"Neutral\") for i in range(n_agents)]\n",
    "        # normalize decisions (capitalization)\n",
    "        decisions_clean = [d.strip().title() if isinstance(d, str) else \"Neutral\" for d in decisions]\n",
    "        counts = {}\n",
    "        for d in decisions_clean:\n",
    "            counts[d] = counts.get(d, 0) + 1\n",
    "        # find top decision\n",
    "        top_decision, top_count = None, 0\n",
    "        for k, v in counts.items():\n",
    "            if v > top_count:\n",
    "                top_decision, top_count = k, v\n",
    "        majority_reached = (top_count > (2/3) * n_agents)\n",
    "        # Update the last n_agents logs entries (phase1 rows) with majority info\n",
    "        for i in range(len(logs) - n_agents, len(logs)):\n",
    "            logs[i][\"majority_decision\"] = top_decision\n",
    "            logs[i][\"majority_count\"] = top_count\n",
    "            logs[i][\"majority_reached\"] = bool(majority_reached)\n",
    "\n",
    "        # -------------------------\n",
    "        # Phase 2: give everyone the Phase1 responses (including themselves), and present next claim\n",
    "        # -------------------------\n",
    "        next_claim = claims[(t + 1) % len(claims)]\n",
    "        next_claim_id = next_claim.get(\"claim_id\")\n",
    "        next_claim_text = str(next_claim.get(\"claim_text\"))\n",
    "\n",
    "        # Build a textual summary of phase1 responses to pass to agents\n",
    "        phase1_summary_lines = []\n",
    "        for idx in range(n_agents):\n",
    "            r = phase1_responses[idx][\"llm_response_struct\"]\n",
    "            name = phase1_responses[idx][\"name\"]\n",
    "            cd = r.get(\"claim_decision\", \"Neutral\")\n",
    "            cd_reason = r.get(\"claim_decision_reason\", \"\")\n",
    "            cb = r.get(\"climateChange_belief\", \"Neutral\")\n",
    "            cb_reason = r.get(\"climateChange_belief_reason\", \"\")\n",
    "            phase1_summary_lines.append(\n",
    "                f\"{name}: claim_decision='{cd}' reason='{cd_reason}' | belief='{cb}' reason='{cb_reason}'\"\n",
    "            )\n",
    "        phase1_summary_text = \"\\n\".join(phase1_summary_lines)\n",
    "\n",
    "        for agent in tqdm(agents, desc=f\"Time {t} Phase 2\", leave=False):\n",
    "            system_msg = GROUP_SYSTEM_TMPL.replace(\"{PERSONA_DESCRIPTION}\", agent.persona_desc)\n",
    "\n",
    "            neighbor_opinions = summarize_neighbors_all(agent.idx, agents, G)\n",
    "            \n",
    "            user_prompt = EACH_ROUND_PROMPT.format(\n",
    "                CLAIM_TEXT=next_claim_text,\n",
    "                NEIGHBOUR_ID=\"\",\n",
    "                NEIGHBOR_OPINIONS=neighbor_opinions\n",
    "            )\n",
    "\n",
    "\n",
    "            raw = chat_once(\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                system_msg=system_msg,\n",
    "                user_msg=user_prompt\n",
    "            )\n",
    "\n",
    "            parsed = coerce_json(raw)\n",
    "            new_belief = stance_to_numeric(parsed.get(\"climateChange_belief\", \"Neutral\"))\n",
    "            agent.current_belief = new_belief\n",
    "\n",
    "            # append to history\n",
    "            agent.history.append({\n",
    "                \"time\": t,\n",
    "                \"phase\": 2,\n",
    "                \"llm_response\": parsed,\n",
    "                \"llm_response_struct\": parsed,\n",
    "                \"llm_response_raw\": raw,\n",
    "                \"claim_id\": next_claim_id,\n",
    "                \"claim_text\": next_claim_text,\n",
    "                \"received_phase1_summary\": phase1_summary_text,\n",
    "            })\n",
    "\n",
    "            # log phase2 row\n",
    "            logs.append({\n",
    "                \"time\": t,\n",
    "                \"phase\": 2,\n",
    "                \"persona_id\": agent.persona_id,\n",
    "                \"agent_idx\": agent.idx,\n",
    "                \"name\": agent.name,\n",
    "                \"claim_id\": next_claim_id,\n",
    "                \"claim_text\": next_claim_text,\n",
    "                \"graph_type\": graph_type,\n",
    "                \"claim_decision\": parsed.get(\"claim_decision\"),\n",
    "                \"claim_decision_reason\": parsed.get(\"claim_decision_reason\"),\n",
    "                \"climateChange_belief\": parsed.get(\"climateChange_belief\"),\n",
    "                \"climateChange_belief_reason\": parsed.get(\"climateChange_belief_reason\"),\n",
    "                \"belief_numeric\": agent.current_belief,\n",
    "                \"llm_response_raw\": raw,\n",
    "                # include the Phase 1 majority for convenience\n",
    "                \"phase1_majority_decision\": top_decision,\n",
    "                \"phase1_majority_count\": top_count,\n",
    "                \"phase1_majority_reached\": bool(majority_reached),\n",
    "            })\n",
    "\n",
    "    df_logs = pd.DataFrame(logs)\n",
    "    return df_logs\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. Polarization metrics\n",
    "# ============================================================\n",
    "\n",
    "def compute_polarization_metrics(df_logs: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute simple polarization metrics per time step from df_logs (we aggregate by time, using phase 2 belief values if available).\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    # we compute metrics per time step using the phase 2 belief where possible, otherwise phase 1\n",
    "    for t, df_t in df_logs.groupby(\"time\"):\n",
    "        # prefer phase 2 rows to get updated beliefs\n",
    "        df_pref = df_t[df_t[\"phase\"] == 2]\n",
    "        if df_pref.empty:\n",
    "            df_pref = df_t\n",
    "\n",
    "        beliefs = df_pref[\"belief_numeric\"].fillna(0).values.reshape(-1, 1)\n",
    "        mean = float(beliefs.mean())\n",
    "        std = float(beliefs.std())\n",
    "        frac_extreme = float(np.mean(np.abs(beliefs) > 0.8))\n",
    "\n",
    "        # cluster gap (two-cluster separation)\n",
    "        if len(beliefs) >= 4:\n",
    "            km = KMeans(n_clusters=2, n_init=10, random_state=0)\n",
    "            km.fit(beliefs)\n",
    "            m1, m2 = km.cluster_centers_.flatten()\n",
    "            cluster_gap = float(abs(m1 - m2))\n",
    "        else:\n",
    "            cluster_gap = float(\"nan\")\n",
    "\n",
    "        metrics.append({\n",
    "            \"time\": t,\n",
    "            \"mean_belief\": mean,\n",
    "            \"std_belief\": std,\n",
    "            \"frac_extreme\": frac_extreme,\n",
    "            \"cluster_gap\": cluster_gap,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. CLI main\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Multi-agent climate polarization simulation with LLM personas.\")\n",
    "    parser.add_argument(\"--model\", required=True, help=\"Ollama model name\")\n",
    "    parser.add_argument(\"--personas\", required=True, help=\"Path to personas CSV file\")\n",
    "    parser.add_argument(\"--claims\", required=True, help=\"Path to claims CSV or JSON file\")\n",
    "    parser.add_argument(\n",
    "        \"--claims_format\",\n",
    "        choices=[\"csv\", \"json\"],\n",
    "        default=\"csv\",\n",
    "        help=\"Format of claims file\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--graph_type\",\n",
    "        choices=[\"fully_connected\", \"random\", \"small_world\"],\n",
    "        default=\"fully_connected\",  # explicit default fully connected\n",
    "        help=\"Interaction graph type\",\n",
    "    )\n",
    "    parser.add_argument(\"--steps\", type=int, default=10, help=\"Number of time steps\")\n",
    "    parser.add_argument(\"--temperature\", type=float, default=0.2, help=\"Sampling temperature\")\n",
    "    parser.add_argument(\"--avg_degree\", type=int, default=4, help=\"Avg degree for random graph\")\n",
    "    parser.add_argument(\"--small_world_k\", type=int, default=4, help=\"k for small-world\")\n",
    "    parser.add_argument(\"--small_world_beta\", type=float, default=0.1, help=\"beta for small-world\")\n",
    "    parser.add_argument(\"--n_personas\", type=int, default=None, help=\"Subset of personas\")\n",
    "    parser.add_argument(\"--n_claims\", type=int, default=50, help=\"Number of claims to use\")\n",
    "    parser.add_argument(\"--balanced_claims\", action=\"store_true\",\n",
    "                        help=\"If set, balance SUPPORTS / REFUTES (only for labeled datasets)\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "    parser.add_argument(\"--out_prefix\", default=\"group_polarization\", help=\"Output prefix\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Personas\n",
    "    personas_df = pd.read_csv(args.personas)\n",
    "    if args.n_personas is not None:\n",
    "        personas_df = personas_df.sample(\n",
    "            n=min(args.n_personas, len(personas_df)),\n",
    "            random_state=args.seed)\n",
    "\n",
    "    # Claims\n",
    "    if args.claims_format == \"csv\":\n",
    "        claims = load_claims_with_label_CSV(args.claims)\n",
    "    else:\n",
    "        claims = load_claims_with_label_JSON(args.claims)\n",
    "\n",
    "    if args.balanced_claims:\n",
    "        claims = filter_balanced_claims(claims, n_each=args.n_claims // 2)\n",
    "    else:\n",
    "        # Just sample without balancing\n",
    "        if args.n_claims is not None and args.n_claims < len(claims):\n",
    "            claims = (\n",
    "                pd.DataFrame(claims)\n",
    "                .sample(n=args.n_claims, random_state=args.seed)\n",
    "                .to_dict(orient=\"records\"))\n",
    "\n",
    "    print(f\"Loaded {len(personas_df)} personas and {len(claims)} claims.\")\n",
    "\n",
    "    df_logs = simulate_polarization(\n",
    "        model=args.model,\n",
    "        personas_df=personas_df,\n",
    "        claims=claims,\n",
    "        graph_type=args.graph_type,\n",
    "        steps=args.steps,\n",
    "        temperature=args.temperature,\n",
    "        avg_degree=args.avg_degree,\n",
    "        small_world_k=args.small_world_k,\n",
    "        small_world_beta=args.small_world_beta,\n",
    "        seed=args.seed,\n",
    "    )\n",
    "\n",
    "    df_metrics = compute_polarization_metrics(df_logs)\n",
    "\n",
    "    out_dir = Path(\"outputs\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logs_path = out_dir / f\"{args.out_prefix}_logs.jsonl\"\n",
    "    metrics_path = out_dir / f\"{args.out_prefix}_metrics.csv\"\n",
    "\n",
    "    # Save logs as JSONL\n",
    "    with logs_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in df_logs.iterrows():\n",
    "            f.write(json.dumps(row.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # Save metrics\n",
    "    df_metrics.to_csv(metrics_path, index=False)\n",
    "\n",
    "    print(f\"Saved logs to {logs_path}\")\n",
    "    print(f\"Saved metrics to {metrics_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4e6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMBiases_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
