{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe68210-4a20-4266-9ef2-1d2e99c2b5f7",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd5cd9b-d555-46e5-8edb-5d41658296bf",
   "metadata": {},
   "source": [
    "First, we load the result file containing the experimental outputs.\n",
    "\n",
    "The dataset should include the following columns:\n",
    "\n",
    "| Column Name | Description |\n",
    "|--------------|-------------|\n",
    "| **PersonaID** | Unique identifier of each persona. |\n",
    "| **BeliefClimateExists** | Persona’s prior belief (1–5 scale) on whether climate change exists. |\n",
    "| **ClaimID** | Identifier of the presented claim. |\n",
    "| **ClaimStanceLabel** | Whether the claim *supports* or *refutes* the existence of climate change. |\n",
    "| **EvidencesVerdict** | Strength of evidence behind the claim (`SUPPORTS`, `REFUTES`, or `NOT_ENOUGH_INFO`). |\n",
    "| **Evidence** | Text of the evidence used for the verdict. |\n",
    "| **ClaimEntropy** | Information entropy of the claim (uncertainty measure). |\n",
    "| **ModelDecisionOfClaim** | Model’s final decision on the claim (`Accept`, `Refute`, `Neutral`). |\n",
    "| **ModelDecisionOfClaim_Reason** | Model’s reasoning behind its decision. |\n",
    "| **ModelBeliefClimateExists** | Updated belief (1–5 scale) after reading the claim. |\n",
    "| **ModelBeliefClimateExists_Reason** | Model’s reasoning behind the updated belief. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f79a00-9ac6-42d1-8a72-f630dc4cd4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Change the file name here\n",
    "df = pd.read_csv(\"data/prompts_level_3_with_model_outputs.csv\")\n",
    "\n",
    "# Filter out invalid rows\n",
    "mask = (\n",
    "    (df[\"ModelDecisionOfClaim_Reason\"].notna() & ~df[\"ModelDecisionOfClaim_Reason\"].str.contains(\"model error\", case=False, na=False)) |\n",
    "    (df[\"ModelBeliefClimateExists_Reason\"].notna() & ~df[\"ModelBeliefClimateExists_Reason\"].str.contains(\"model error\", case=False, na=False))\n",
    ")\n",
    "df = df[mask]\n",
    "\n",
    "# Map the textual responses to numbers\n",
    "df[\"BeliefClimateExists_num\"] = df[\"BeliefClimateExists\"].map({\n",
    "    \"Strongly disagree\": -2, \n",
    "    \"Strongly Disagree\": -2,\n",
    "    \"Slightly disagree\": -1,\n",
    "    \"Slightly Disagree\": -1,\n",
    "    \"Neutral\": 0, \n",
    "    \"Slightly agree\": 1,\n",
    "    \"Slightly Agree\": 1,\n",
    "    \"Strongly agree\": 2,\n",
    "    \"Strongly Agree\": 2\n",
    "})\n",
    "df[\"ClaimStanceLabel_num\"] = df[\"ClaimStanceLabel\"].map({\"REFUTES\": -1, \"SUPPORTS\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715bacde-b8b2-4e1f-bce1-96c409c76d16",
   "metadata": {},
   "source": [
    "Alignment captures whether the user’s prior belief matches the claim stance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70e7dbd-f7f3-4f53-b1f1-b0308bdf368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment in {+1/+2, 0, -1/-2}\n",
    "# +1/+2 if belief sign and claim sign match (both >0 or both <0), -1/-2 otherwise.\n",
    "# Neutral (belief==0) is assigned 0 for alignment\n",
    "belief_sign = df[\"BeliefClimateExists_num\"].apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "claim_sign  = df[\"ClaimStanceLabel_num\"]  # already ±1\n",
    "df[\"Alignment\"] = belief_sign * claim_sign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc51d18-5a83-47df-bc42-2e69e55efe3e",
   "metadata": {},
   "source": [
    "Simplify the model’s decision to a binary outcome (accept = 1, refute = 0). You can treat “Neutral” as missing or as 0.5 if you want to retain it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb6c5d-e068-4bb8-ae62-f6904319de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Decision_binary\"] = df[\"ModelDecisionOfClaim\"].map({\"Accept\": 1, \"Refute\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25889160-75ca-4d03-930a-4b91bd578f24",
   "metadata": {},
   "source": [
    "Then comes the logistic regression.\n",
    "\n",
    "We model whether the model/persona **accepts** a displayed claim as a function of  \n",
    "the alignment between its prior belief and the claim stance, and the strength of evidence.\n",
    "Instead of a single interaction term, we explicitly include four combinations of\n",
    "alignment (\\( \\text{Alignment}^+ \\), \\( \\text{Alignment}^- \\)) and evidence polarity\n",
    "(\\( \\text{EvidenceStrength}^+ \\), \\( \\text{EvidenceStrength}^- \\)):\n",
    "\n",
    "$$\n",
    "\\text{logit}\\big(P(\\text{Accept})\\big)\n",
    "= \\beta_0\n",
    "+ \\beta_1 \\text{Alignment}\n",
    "+ \\beta_2 \\text{EvidenceStrength}\n",
    "+ \\beta_{3} I(\\text{Alignment}^+, \\text{EvidenceStrength}^+)\n",
    "+ \\beta_{4} I(\\text{Alignment}^-, \\text{EvidenceStrength}^+)\n",
    "+ \\beta_{5} I(\\text{Alignment}^+, \\text{EvidenceStrength}^-)\n",
    "+ \\beta_{6} I(\\text{Alignment}^-, \\text{EvidenceStrength}^-)\n",
    "$$\n",
    "\n",
    "**Where**\n",
    "\n",
    "- **$Accept$**: Binary outcome (1 = Accept, 0 = Refute)  \n",
    "- **$\\text{Alignment}^+$ / $\\text{Alignment}^-$**: Whether the claim aligns or conflicts with the model/persona’s prior belief  \n",
    "- **$\\text{EvidenceStrength}^+$ / $\\text{EvidenceStrength}^-$**: Whether the evidence is strong or weak  \n",
    "- **$I(\\cdot,\\cdot)$**: Indicator variables for each specific alignment–evidence combination  \n",
    "- **$\\beta_1,\\beta_2$**: Main effects of alignment and evidence polarity  \n",
    "- **$\\beta_{3}$–$\\beta_{6}$**: Cell-specific adjustments capturing deviations from the main effects  \n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "- **$\\beta_{3}$ > 0** → Aligned claims with *strong evidence* are especially likely to be accepted  \n",
    "  (*strong confirmation bias under favorable evidence*)  \n",
    "- **$\\beta_{4}$ > 0** → Misaligned claims with *strong evidence* still tend to be accepted  \n",
    "  (*Will the model be swayed by strong evidence*)  \n",
    "- **$\\beta_{5}$ < 0** → Aligned claims with *weak evidence* are less likely to be accepted\n",
    "- **$\\beta_{6}$ < 0** → Misaligned claims with *weak evidence* are readily rejected  \n",
    "  (*reinforced disconfirmation*)  \n",
    "\n",
    "This formulation allows us to examine how confirmation bias manifests differently\n",
    "across favorable and unfavorable evidence contexts rather than relying on a single\n",
    "interaction term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d467dc7b-7578-484b-9779-c7892eb2a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Evidence strength in {-1,+1}, -1 means weak evidence, 1 means strong evidence\n",
    "# Both refute and not enough info are considered weak evidence\n",
    "try:\n",
    "    df[\"EvidenceStrength_num\"] = df[\"EvidencesVerdict\"].map({\n",
    "        \"REFUTES\": -1, \"NOT_ENOUGH_INFO\": -1, \"SUPPORTS\": 1\n",
    "    })\n",
    "except:\n",
    "    df[\"ClaimID\"] = df[\"ClaimID\"].astype(str)\n",
    "    evidence_df = pd.read_json(\"data/claims_EL3.json\")\n",
    "    evidence_df[\"claim_id\"] = evidence_df[\"claim_id\"].astype(str)\n",
    "    df = df.merge(\n",
    "        evidence_df[[\"claim_id\", \"claim_label\"]],\n",
    "        left_on=\"ClaimID\",\n",
    "        right_on=\"claim_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    df[\"EvidenceStrength_num\"] = df[\"claim_label\"].map({\n",
    "        \"REFUTES\": -1, \"NOT_ENOUGH_INFO\": -1, \"SUPPORTS\": 1\n",
    "    })\n",
    "\n",
    "# Four interaction variables\n",
    "df[\"I_A1_E1\"]  = ((df[\"Alignment\"] ==  1) & (df[\"EvidenceStrength_num\"] ==  1)).astype(int)\n",
    "df[\"I_A_1_E1\"] = ((df[\"Alignment\"] == -1) & (df[\"EvidenceStrength_num\"] ==  1)).astype(int)\n",
    "df[\"I_A1_E_1\"] = ((df[\"Alignment\"] ==  1) & (df[\"EvidenceStrength_num\"] == -1)).astype(int)\n",
    "df[\"I_A_1_E_1\"]= ((df[\"Alignment\"] == -1) & (df[\"EvidenceStrength_num\"] == -1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc42a0c-4afb-4e8b-84cb-41c9be774714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.618054\n",
      "         Iterations 5\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:        Decision_binary   No. Observations:                31504\n",
      "Model:                          Logit   Df Residuals:                    31499\n",
      "Method:                           MLE   Df Model:                            4\n",
      "Date:                Thu, 30 Oct 2025   Pseudo R-squ.:                  0.1027\n",
      "Time:                        22:43:20   Log-Likelihood:                -19471.\n",
      "converged:                       True   LL-Null:                       -21700.\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.6237      0.027     22.786      0.000       0.570       0.677\n",
      "I_A1_E1        0.9523      0.048     19.975      0.000       0.859       1.046\n",
      "I_A_1_E1       0.4034      0.044      9.111      0.000       0.317       0.490\n",
      "I_A1_E_1      -0.9408      0.035    -27.022      0.000      -1.009      -0.873\n",
      "I_A_1_E_1     -1.2791      0.036    -35.309      0.000      -1.350      -1.208\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Fit logistic regression\n",
    "use = df.dropna(subset=[\"Decision_binary\", \"Alignment\", \"EvidenceStrength_num\"])\n",
    "model = smf.logit(\n",
    "    \"Decision_binary ~ I_A1_E1 + I_A_1_E1 + I_A1_E_1 + I_A_1_E_1\",\n",
    "    data=use\n",
    ").fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f43ea6e",
   "metadata": {},
   "source": [
    "Proportion inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f86c0-e398-400f-8e47-d06754500230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alignment  EvidenceStrength_num\n",
       "-1         -1                      0.341779\n",
       "            1                      0.736359\n",
       " 0         -1                      0.497667\n",
       "            1                      0.901434\n",
       " 1         -1                      0.421397\n",
       "            1                      0.828646\n",
       "Name: Decision_binary, dtype: float64"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use.groupby([\"Alignment\", \"EvidenceStrength_num\"])[\"Decision_binary\"].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.1 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
